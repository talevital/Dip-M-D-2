# Projet ETL Python - Documentation Compl√®te

## üìã Table des Mati√®res

1. [Installation](#installation)
2. [Structure du Projet](#structure-du-projet)
3. [Utilisation](#utilisation)
4. [Modules et Fonctionnalit√©s](#modules-et-fonctionnalit√©s)
5. [Exemples d'Utilisation](#exemples-dutilisation)
6. [Tests](#tests)
7. [Extension du Projet](#extension-du-projet)
8. [D√©pannage](#d√©pannage)

## üöÄ Installation

### Pr√©requis

- Python 3.8+
- PostgreSQL (optionnel pour le chargement)
- pip

### Installation des D√©pendances

```bash
# Cloner ou t√©l√©charger le projet
cd etl_project

# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### Configuration de la Base de Donn√©es (Optionnel)

Pour utiliser le chargement PostgreSQL, configurez la variable d'environnement :

```bash
# Linux/Mac
export DATABASE_URL="postgresql://username:password@localhost:5432/database_name"

# Windows
set DATABASE_URL=postgresql://username:password@localhost:5432/database_name
```

## üìÅ Structure du Projet

```
etl_project/
‚îÇ
‚îú‚îÄ etl/                          # Modules ETL principaux
‚îÇ   ‚îú‚îÄ extract/                  # Extraction de donn√©es
‚îÇ   ‚îÇ   ‚îî‚îÄ csv_extractor.py      # Extracteur CSV avec validation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ transform/                # Transformation de donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ clean_data.py         # Nettoyage (valeurs manquantes, doublons, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ normalize_data.py     # Normalisation et standardisation
‚îÇ   ‚îÇ   ‚îî‚îÄ enrich_data.py        # Enrichissement et cr√©ation de features
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ load/                     # Chargement de donn√©es
‚îÇ       ‚îî‚îÄ load_postgres.py      # Chargement PostgreSQL avec SQLAlchemy
‚îÇ
‚îú‚îÄ utils/                        # Utilitaires
‚îÇ   ‚îî‚îÄ helpers.py               # Profilage et validation des donn√©es
‚îÇ
‚îú‚îÄ tests/                        # Tests unitaires
‚îÇ   ‚îî‚îÄ test_etl.py              # Tests du pipeline ETL
‚îÇ
‚îú‚îÄ main.py                       # Point d'entr√©e principal
‚îú‚îÄ requirements.txt              # D√©pendances Python
‚îî‚îÄ README.md                     # Cette documentation
```

## üéØ Utilisation

### D√©marrage Rapide

```bash
# Option 1: Script unifi√© (recommand√©)
python start.py api    # D√©marre l'API FastAPI
python start.py etl    # Lance le pipeline ETL

# Option 2: Scripts individuels
python start_api.py    # D√©marre l'API FastAPI
python start_etl.py    # Lance le pipeline ETL

# Option 3: Commandes directes
python main.py         # Pipeline ETL avec donn√©es d'exemple
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000  # API FastAPI
```

### üöÄ D√©marrage de l'API

```bash
# Depuis le r√©pertoire etl_project/
python start.py api

# L'API sera disponible sur:
# - http://localhost:8000
# - Documentation: http://localhost:8000/docs
# - Interface interactive: http://localhost:8000/redoc
```

### Utilisation Programm√©e

```python
from etl.extract.csv_extractor import CSVExtractor
from etl.transform.clean_data import DataCleaner
from etl.transform.normalize_data import DataNormalizer
from etl.transform.enrich_data import DataEnricher
from etl.load.load_postgres import PostgreSQLLoader
from utils.helpers import DataProfiler

# 1. Extraction
extractor = CSVExtractor()
df = extractor.read_csv('your_data.csv')

# 2. Nettoyage
cleaner = DataCleaner()
df_clean = cleaner.clean_data(df)

# 3. Normalisation
normalizer = DataNormalizer()
df_normalized = normalizer.normalize_data(df_clean)

# 4. Enrichissement
enricher = DataEnricher()
df_enriched = enricher.enrich_data(df_normalized)

# 5. Chargement (si PostgreSQL configur√©)
if os.getenv('DATABASE_URL'):
    loader = PostgreSQLLoader(os.getenv('DATABASE_URL'))
    loader.load_data(df_enriched, 'table_name')
```

## üîß Modules et Fonctionnalit√©s

### üì• Extraction (`etl/extract/`)

#### CSVExtractor
- **Lecture de fichiers CSV** avec gestion d'erreurs
- **Validation des donn√©es** (structure, contenu)
- **Logging d√©taill√©** des op√©rations
- **Gestion des encodages** et formats

```python
extractor = CSVExtractor()
df = extractor.read_csv('data.csv', encoding='utf-8')
validation = extractor.validate_csv(df)
```

### üîÑ Transformation (`etl/transform/`)

#### DataCleaner (`clean_data.py`)
- **Gestion des valeurs manquantes** : imputation, suppression, interpolation
- **Suppression des doublons** avec options configurables
- **Traitement des valeurs aberrantes** : winsorisation, IQR, z-score
- **Correction des incoh√©rences** : standardisation des cha√Ænes

```python
cleaner = DataCleaner()
df_clean = cleaner.clean_data(
    df,
    missing_strategy='fill',      # 'drop', 'fill', 'interpolate', 'group_fill'
    remove_duplicates=True,
    handle_outliers=True,
    fix_inconsistencies=True
)
```

#### DataNormalizer (`normalize_data.py`)
- **Normalisation num√©rique** : StandardScaler, MinMaxScaler, RobustScaler
- **Standardisation cat√©gorielle** : label encoding, one-hot encoding, frequency encoding
- **Normalisation des dates** : extraction de composants temporels
- **Transformations personnalis√©es** : log, Box-Cox

```python
normalizer = DataNormalizer()
df_normalized = normalizer.normalize_data(
    df,
    numerical_method='standard',   # 'standard', 'minmax', 'robust', 'log', 'boxcox'
    categorical_method='label',    # 'label', 'onehot', 'frequency'
    normalize_dates=True
)
```

#### DataEnricher (`enrich_data.py`)
- **Colonnes conditionnelles** : calculs bas√©s sur des fonctions
- **Features agr√©g√©es** : groupements et agr√©gations
- **Features temporelles** : extraction de composants de dates
- **Features d'interaction** : op√©rations entre colonnes
- **Binning** : discr√©tisation de variables continues

```python
enricher = DataEnricher()

# Colonnes conditionnelles
conditions = {
    'pib_par_habitant': lambda df: df['pib'] / df['population'],
    'balance_commerciale': lambda df: df['export'] - df['import']
}

# Agr√©gations
aggregations = {
    'group_by': 'pays',
    'aggregations': {'pib': ['mean', 'sum'], 'population': ['mean']},
    'prefix': 'agg'
}

df_enriched = enricher.enrich_data(
    df,
    conditional_columns=conditions,
    aggregations=aggregations
)
```

### üì§ Chargement (`etl/load/`)

#### PostgreSQLLoader (`load_postgres.py`)
- **Connexion PostgreSQL** avec SQLAlchemy
- **Cr√©ation automatique de tables** bas√©e sur le DataFrame
- **Chargement optimis√©** par chunks
- **Gestion des types de donn√©es** automatique
- **Ex√©cution de requ√™tes SQL** personnalis√©es

```python
loader = PostgreSQLLoader('postgresql://user:pass@localhost/db')
success = loader.load_data(df, 'table_name', if_exists='replace')
```

### üõ†Ô∏è Utilitaires (`utils/`)

#### DataProfiler (`helpers.py`)
- **Profilage complet** des donn√©es
- **M√©triques de qualit√©** : compl√©tude, coh√©rence, validit√©
- **Analyse m√©moire** et performance
- **Sauvegarde/chargement** de profils JSON
- **Comparaison de profils** pour d√©tecter les changements

```python
profiler = DataProfiler()
profile = profiler.profile_dataframe(df, detailed=True)
profiler.save_profile(profile, 'profile.json')
```

## üìä Exemples d'Utilisation

### Exemple 1 : Pipeline Complet pour Donn√©es √âconomiques

```python
import pandas as pd
from etl.extract.csv_extractor import CSVExtractor
from etl.transform.clean_data import DataCleaner
from etl.transform.normalize_data import DataNormalizer
from etl.transform.enrich_data import DataEnricher
from utils.helpers import DataProfiler

# 1. Extraction
extractor = CSVExtractor()
df = extractor.read_csv('economic_data.csv')

# 2. Profilage initial
profiler = DataProfiler()
profile_initial = profiler.profile_dataframe(df)

# 3. Nettoyage
cleaner = DataCleaner()
df_clean = cleaner.clean_data(
    df,
    missing_strategy='group_fill',
    group_by='pays',
    handle_outliers='winsorize'
)

# 4. Normalisation
normalizer = DataNormalizer()
df_normalized = normalizer.normalize_data(
    df_clean,
    numerical_method='robust',
    categorical_method='onehot'
)

# 5. Enrichissement
enricher = DataEnricher()

# Cr√©ation de features √©conomiques
conditions = {
    'pib_par_habitant': lambda df: df['pib_millions'] / df['population_millions'],
    'balance_commerciale': lambda df: df['export_millions'] - df['import_millions'],
    'taux_ouverture': lambda df: (df['export_millions'] + df['import_millions']) / df['pib_millions'] * 100,
    'pays_riche': lambda df: df['pib_millions'] > df['pib_millions'].quantile(0.75)
}

# Agr√©gations par pays
aggregations = {
    'group_by': 'pays',
    'aggregations': {
        'pib_millions': ['mean', 'sum', 'std'],
        'population_millions': ['mean', 'sum'],
        'inflation_taux': ['mean', 'std']
    },
    'prefix': 'agg'
}

df_enriched = enricher.enrich_data(
    df_normalized,
    conditional_columns=conditions,
    aggregations=aggregations
)

# 6. Profilage final
profile_final = profiler.profile_dataframe(df_enriched)
comparison = profiler.compare_profiles(profile_initial, profile_final)

print(f"Donn√©es enrichies: {len(df_enriched)} lignes, {len(df_enriched.columns)} colonnes")
print(f"Changements d√©tect√©s: {comparison['changes_detected']}")
```

### Exemple 2 : Traitement de Donn√©es Temporelles

```python
# Donn√©es avec colonnes de dates
df = pd.DataFrame({
    'date': pd.date_range('2023-01-01', periods=100, freq='D'),
    'valeur': np.random.normal(100, 20, 100),
    'categorie': np.random.choice(['A', 'B', 'C'], 100)
})

# Enrichissement avec features temporelles
enricher = DataEnricher()
time_features = {
    'date_column': 'date',
    'features': ['year', 'month', 'quarter', 'day_of_week', 'is_weekend']
}

df_enriched = enricher.enrich_data(df, time_features=time_features)

# Agr√©gations temporelles
aggregations = {
    'group_by': ['year', 'month'],
    'aggregations': {'valeur': ['mean', 'sum', 'count']},
    'prefix': 'monthly'
}

df_final = enricher.enrich_data(df_enriched, aggregations=aggregations)
```

## üß™ Tests

### Ex√©cution des Tests

```bash
# Installer pytest si pas d√©j√† fait
pip install pytest

# Ex√©cuter tous les tests
pytest tests/

# Ex√©cuter un test sp√©cifique
pytest tests/test_etl.py::TestETLPipeline::test_csv_extractor

# Ex√©cuter avec couverture
pytest --cov=etl tests/
```

### Tests Disponibles

- **test_csv_extractor** : Test de l'extracteur CSV
- **test_data_cleaner** : Test du nettoyeur de donn√©es
- **test_data_normalizer** : Test du normaliseur
- **test_data_enricher** : Test de l'enrichisseur
- **test_data_profiler** : Test du profileur
- **test_end_to_end_pipeline** : Test du pipeline complet

## üîß Extension du Projet

### Ajout d'un Nouveau Format d'Extraction

1. **Cr√©er un nouveau fichier** dans `etl/extract/` :

```python
# etl/extract/excel_extractor.py
import pandas as pd
from loguru import logger

class ExcelExtractor:
    def __init__(self):
        self.logger = logger
    
    def read_excel(self, file_path: str, **kwargs) -> pd.DataFrame:
        try:
            self.logger.info(f"Lecture du fichier Excel: {file_path}")
            df = pd.read_excel(file_path, **kwargs)
            self.logger.info(f"Fichier Excel lu: {len(df)} lignes")
            return df
        except Exception as e:
            self.logger.error(f"Erreur lecture Excel: {e}")
            raise
```

2. **Ajouter les tests** dans `tests/test_etl.py` :

```python
def test_excel_extractor(self, tmp_path):
    # Test du nouvel extracteur
    pass
```

### Ajout d'une Nouvelle M√©thode de Transformation

1. **√âtendre la classe existante** ou cr√©er une nouvelle :

```python
# Dans etl/transform/clean_data.py
def handle_text_normalization(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """Normalise le texte (majuscules, accents, etc.)"""
    df_clean = df.copy()
    for col in columns:
        if col in df.columns:
            df_clean[col] = df_clean[col].str.lower().str.strip()
    return df_clean
```

2. **Mettre √† jour la documentation** et les tests.

### Ajout d'une Nouvelle Base de Donn√©es

1. **Cr√©er un nouveau chargeur** dans `etl/load/` :

```python
# etl/load/load_mysql.py
from sqlalchemy import create_engine

class MySQLoader:
    def __init__(self, connection_string: str):
        self.engine = create_engine(connection_string)
    
    def load_data(self, df: pd.DataFrame, table_name: str) -> bool:
        # Impl√©mentation sp√©cifique √† MySQL
        pass
```

## üîç D√©pannage

### Probl√®mes Courants

#### Erreur de Connexion PostgreSQL
```
Error: connection to server at "localhost" failed
```
**Solution** : V√©rifier que PostgreSQL est d√©marr√© et que les param√®tres de connexion sont corrects.

#### Erreur de M√©moire
```
MemoryError: Unable to allocate array
```
**Solution** : Utiliser le chargement par chunks ou r√©duire la taille des donn√©es.

#### Erreur de Type de Donn√©es
```
TypeError: Cannot convert string to float
```
**Solution** : V√©rifier les types de donn√©es dans le CSV et ajuster la strat√©gie de nettoyage.

### Logs et Debugging

Le projet utilise `loguru` pour le logging. Les logs sont sauvegard√©s dans `etl_pipeline.log`.

```python
from loguru import logger

# Activer le debug
logger.add("debug.log", level="DEBUG")

# Logs personnalis√©s
logger.info("D√©but du traitement")
logger.warning("Valeurs manquantes d√©tect√©es")
logger.error("Erreur critique")
```

### Performance

#### Optimisations Recommand√©es

1. **Chargement par chunks** pour les gros fichiers
2. **Types de donn√©es optimis√©s** (int32 au lieu de int64)
3. **Parall√©lisation** pour les transformations lourdes
4. **Indexation** des bases de donn√©es

#### Monitoring

```python
import time
from utils.helpers import DataProfiler

start_time = time.time()

# Votre pipeline ETL
df_processed = process_data(df)

end_time = time.time()
print(f"Temps de traitement: {end_time - start_time:.2f} secondes")

# Profilage m√©moire
profiler = DataProfiler()
profile = profiler.profile_dataframe(df_processed)
print(f"Utilisation m√©moire: {profile['memory_usage']['total_memory_mb']:.2f} MB")
```

## üìö Ressources Additionnelles

### Documentation des Biblioth√®ques

- [Pandas](https://pandas.pydata.org/docs/) - Manipulation de donn√©es
- [SQLAlchemy](https://docs.sqlalchemy.org/) - ORM et connexions DB
- [Scikit-learn](https://scikit-learn.org/stable/) - Normalisation et preprocessing
- [Loguru](https://loguru.readthedocs.io/) - Logging avanc√©

### Bonnes Pratiques ETL

1. **Validation des donn√©es** √† chaque √©tape
2. **Logging d√©taill√©** pour le debugging
3. **Gestion d'erreurs** robuste
4. **Tests unitaires** complets
5. **Documentation** des transformations
6. **Monitoring** des performances

---

**Projet ETL Python** - D√©velopp√© pour l'analyse √©conomique et la visualisation de donn√©es üöÄ
