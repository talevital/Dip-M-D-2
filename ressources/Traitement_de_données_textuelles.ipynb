{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d74282",
   "metadata": {},
   "source": [
    "1. Titre du Projet\n",
    "Harmonisation Automatique des Noms de Clients avec le Machine Learning\n",
    "\n",
    "2. Méthodologie Détaillée\n",
    "\n",
    "Étape 1 : Compréhension et Préparation des Données\n",
    "\n",
    "Collecte des Données : Importer le fichier contenant les données des clients, s'assurer que la colonne des noms de clients est bien identifiée.\n",
    "Exploration des Données : Analyser les différentes variantes des noms de clients pour comprendre la nature et l'étendue du problème. Identifier les patterns communs et les anomalies.\n",
    "Prétraitement des Données : Nettoyer les données pour éliminer les doublons, les valeurs manquantes, et toute autre anomalie apparente.\n",
    "\n",
    "Étape 2 : Extraction des Caractéristiques\n",
    "\n",
    "Normalisation : Appliquer des techniques de normalisation sur les noms pour les uniformiser. Cela inclut :\n",
    "Conversion en minuscules\n",
    "Suppression des accents et des caractères spéciaux\n",
    "Normalisation des espaces\n",
    "Tri des mots dans les noms pour gérer les permutations\n",
    "Tokenization : Diviser chaque nom en tokens (prénoms, noms de famille, initiales, etc.).\n",
    "Encodage des Caractéristiques : Transformer les tokens en vecteurs numériques à l'aide de techniques comme TF-IDF ou des embeddings de mots (Word2Vec, GloVe).\n",
    "\n",
    "Étape 3 : Modélisation\n",
    "\n",
    "Clustering : Utiliser des algorithmes de clustering comme DBSCAN pour regrouper les noms similaires. DBSCAN est choisi pour sa capacité à identifier des clusters de formes variées et à gérer le bruit.\n",
    "Modèle de Correspondance : Entraîner un modèle de correspondance de noms, tel qu'un réseau de neurones ou un modèle basé sur des distances (cosine similarity, Jaccard index, etc.), pour identifier les noms similaires.\n",
    "Supervised Learning : Si des étiquettes de noms harmonisés sont disponibles, utiliser des algorithmes supervisés comme les forêts aléatoires, les réseaux de neurones ou les SVMs pour apprendre les patterns de correspondance.\n",
    "\n",
    "Étape 4 : Post-Traitement et Validation\n",
    "\n",
    "Regroupement et Validation : Regrouper les variantes identifiées par le modèle. Vérifier manuellement un échantillon des résultats pour s'assurer de la précision du modèle.\n",
    "Évaluation du Modèle : Utiliser des métriques d'évaluation comme la précision, le rappel, le F1-score pour mesurer la performance du modèle. Ajuster les paramètres du modèle si nécessaire pour améliorer les résultats.\n",
    "Automatisation du Processus : Une fois le modèle validé, automatiser le processus d'harmonisation des noms pour qu'il puisse être appliqué à de nouveaux ensembles de données sans intervention manuelle.\n",
    "\n",
    "Étape 5 : Implémentation et Suivi\n",
    "\n",
    "Déploiement : Déployer le modèle dans un environnement de production, où il peut traiter les nouvelles entrées de données en temps réel ou par lots.\n",
    "Monitoring : Mettre en place un système de suivi pour surveiller la performance du modèle au fil du temps et détecter toute dégradation de performance.\n",
    "Mise à Jour : Prévoir des mises à jour régulières du modèle pour inclure de nouvelles variantes de noms ou pour ajuster aux évolutions des données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Exemple de données avec plus de variations\n",
    "data = {\n",
    "    'client_names': [\n",
    "        'KOUASSI Franck Ismael', 'KOUASSI Ismael Franck', 'Ismael Franck KOUASSI',\n",
    "        'Franck Ismael KOUASSI', 'KOUASI Franck Ismael', 'KOUSSI Franck Ismael',\n",
    "        'KOUASSI Frack Ismael', 'KOUASSI Franck Ismal', 'Franck KOUASSI',\n",
    "        'Ismael KOUASSI', 'KOUASSI I. Franck'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Chargement des données dans un DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Prétraitement : normalisation des noms\n",
    "def normalize_name(name):\n",
    "    name = name.lower()  # Mettre en minuscules\n",
    "    name = name.replace('.', '')  # Supprimer les points\n",
    "    name = name.replace(',', '')  # Supprimer les virgules\n",
    "    name = name.replace('é', 'e')  # Normaliser les accents\n",
    "    name = name.replace('è', 'e')  # Normaliser les accents\n",
    "    name = ' '.join(sorted(name.split()))  # Trier les mots dans l'ordre alphabétique\n",
    "    return name\n",
    "\n",
    "df['normalized_names'] = df['client_names'].apply(normalize_name)\n",
    "\n",
    "# Vectorisation TF-IDF des noms normalisés\n",
    "vectorizer = TfidfVectorizer().fit_transform(df['normalized_names'])\n",
    "vectors = vectorizer.toarray()\n",
    "\n",
    "# Clustering DBSCAN basé sur la similarité cosinus\n",
    "dbscan = DBSCAN(metric='cosine', eps=0.1, min_samples=1)\n",
    "df['cluster'] = dbscan.fit_predict(vectors)\n",
    "\n",
    "# Fonction pour obtenir le nom le plus fréquent dans chaque cluster\n",
    "def get_representative_name(cluster_df):\n",
    "    return cluster_df['client_names'].mode()[0]\n",
    "\n",
    "# Harmonisation des noms par cluster\n",
    "harmonized_names = df.groupby('cluster').apply(get_representative_name)\n",
    "harmonized_names = harmonized_names.to_dict()\n",
    "\n",
    "# Appliquer les noms harmonisés au DataFrame original\n",
    "df['harmonized_name'] = df['cluster'].map(harmonized_names)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(df[['client_names', 'harmonized_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14289233",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textdistance pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Exemple de données avec plus de variations\n",
    "data = {\n",
    "    'client_names': [\n",
    "        'KOUASSI Franck Ismael', 'KOUASSI Ismael Franck', 'Ismael Franck KOUASSI',\n",
    "        'Franck Ismael KOUASSI', 'KOUASI Franck Ismael', 'KOUSSI Franck Ismael',\n",
    "        'KOUASSI Frack Ismael', 'KOUASSI Franck Ismal', 'Franck KOUASSI',\n",
    "        'Ismael KOUASSI', 'KOUASSI I. Franck'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Chargement des données dans un DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dictionnaire de correction manuelle des erreurs\n",
    "correction_dict = {\n",
    "    'frack': 'franck',\n",
    "    'ismal': 'ismael'\n",
    "}\n",
    "\n",
    "# Fonction de correction des erreurs de frappe\n",
    "def correct_spelling(name, correction_dict):\n",
    "    words = name.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        # Trouver le mot le plus similaire dans le dictionnaire de correction\n",
    "        corrected_word = min(correction_dict.keys(), key=lambda x: textdistance.levenshtein(word.lower(), x))\n",
    "        # Si la distance de Levenshtein est inférieure à un seuil, corriger le mot\n",
    "        if textdistance.levenshtein(word.lower(), corrected_word) <= 2:\n",
    "            corrected_words.append(correction_dict[corrected_word])\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Correction des erreurs dans les noms\n",
    "df['corrected_names'] = df['client_names'].apply(lambda x: correct_spelling(x, correction_dict))\n",
    "\n",
    "# Prétraitement : normalisation des noms\n",
    "def normalize_name(name):\n",
    "    name = name.lower()  # Mettre en minuscules\n",
    "    name = name.replace('.', '')  # Supprimer les points\n",
    "    name = name.replace(',', '')  # Supprimer les virgules\n",
    "    name = name.replace('é', 'e')  # Normaliser les accents\n",
    "    name = name.replace('è', 'e')  # Normaliser les accents\n",
    "    name = ' '.join(sorted(name.split()))  # Trier les mots dans l'ordre alphabétique\n",
    "    return name\n",
    "\n",
    "df['normalized_names'] = df['corrected_names'].apply(normalize_name)\n",
    "\n",
    "# Vectorisation TF-IDF des noms normalisés\n",
    "vectorizer = TfidfVectorizer().fit_transform(df['normalized_names'])\n",
    "vectors = vectorizer.toarray()\n",
    "\n",
    "# Clustering DBSCAN basé sur la similarité cosinus\n",
    "dbscan = DBSCAN(metric='cosine', eps=0.1, min_samples=1)\n",
    "df['cluster'] = dbscan.fit_predict(vectors)\n",
    "\n",
    "# Fonction pour obtenir le nom le plus fréquent dans chaque cluster\n",
    "def get_representative_name(cluster_df):\n",
    "    return cluster_df['client_names'].mode()[0]\n",
    "\n",
    "# Harmonisation des noms par cluster\n",
    "harmonized_names = df.groupby('cluster').apply(get_representative_name)\n",
    "harmonized_names = harmonized_names.to_dict()\n",
    "\n",
    "# Appliquer les noms harmonisés au DataFrame original\n",
    "df['harmonized_name'] = df['cluster'].map(harmonized_names)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(df[['client_names', 'corrected_names', 'harmonized_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c009fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             client_names        corrected_names        harmonized_name\n",
      "0   KOUASSI Franck Ismael  KOUASSI franck ismael  Franck Ismael KOUASSI\n",
      "1   KOUASSI Ismael Franck  KOUASSI ismael franck  Franck Ismael KOUASSI\n",
      "2   Ismael Franck KOUASSI  ismael franck KOUASSI  Franck Ismael KOUASSI\n",
      "3   Franck Ismael KOUASSI  franck ismael KOUASSI  Franck Ismael KOUASSI\n",
      "4    KOUASI Franck Ismael   KOUASI franck ismael   KOUASI Franck Ismael\n",
      "5    KOUSSI Franck Ismael   KOUSSI franck ismael   KOUSSI Franck Ismael\n",
      "6    KOUASSI Frack Ismael  KOUASSI franck ismael  Franck Ismael KOUASSI\n",
      "7    KOUASSI Franck Ismal  KOUASSI franck ismael  Franck Ismael KOUASSI\n",
      "8          Franck KOUASSI         franck KOUASSI         Franck KOUASSI\n",
      "9          Ismael KOUASSI         ismael KOUASSI         Ismael KOUASSI\n",
      "10      KOUASSI I. Franck      KOUASSI I. franck         Franck KOUASSI\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Exemple de données avec plus de variations\n",
    "data = {\n",
    "    'client_names': [\n",
    "        'KOUASSI Franck Ismael', 'KOUASSI Ismael Franck', 'Ismael Franck KOUASSI',\n",
    "        'Franck Ismael KOUASSI', 'KOUASI Franck Ismael', 'KOUSSI Franck Ismael',\n",
    "        'KOUASSI Frack Ismael', 'KOUASSI Franck Ismal', 'Franck KOUASSI',\n",
    "        'Ismael KOUASSI', 'KOUASSI I. Franck'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Chargement des données dans un DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dictionnaire de correction manuelle des erreurs\n",
    "correction_dict = {\n",
    "    'frack': 'franck',\n",
    "    'ismal': 'ismael'\n",
    "}\n",
    "\n",
    "# Fonction de correction des erreurs de frappe\n",
    "def correct_spelling(name, correction_dict):\n",
    "    words = name.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        # Trouver le mot le plus similaire dans le dictionnaire de correction\n",
    "        corrected_word = min(correction_dict.keys(), key=lambda x: textdistance.levenshtein(word.lower(), x))\n",
    "        # Si la distance de Levenshtein est inférieure à un seuil, corriger le mot\n",
    "        if textdistance.levenshtein(word.lower(), corrected_word) <= 2:\n",
    "            corrected_words.append(correction_dict[corrected_word])\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Correction des erreurs dans les noms\n",
    "df['corrected_names'] = df['client_names'].apply(lambda x: correct_spelling(x, correction_dict))\n",
    "\n",
    "# Prétraitement : normalisation des noms\n",
    "def normalize_name(name):\n",
    "    name = name.lower()  # Mettre en minuscules\n",
    "    name = name.replace('.', '')  # Supprimer les points\n",
    "    name = name.replace(',', '')  # Supprimer les virgules\n",
    "    name = name.replace('é', 'e')  # Normaliser les accents\n",
    "    name = name.replace('è', 'e')  # Normaliser les accents\n",
    "    name = ' '.join(sorted(name.split()))  # Trier les mots dans l'ordre alphabétique\n",
    "    return name\n",
    "\n",
    "df['normalized_names'] = df['corrected_names'].apply(normalize_name)\n",
    "\n",
    "# Vectorisation TF-IDF des noms normalisés\n",
    "vectorizer = TfidfVectorizer().fit_transform(df['normalized_names'])\n",
    "vectors = vectorizer.toarray()\n",
    "\n",
    "# Clustering DBSCAN basé sur la similarité cosinus\n",
    "dbscan = DBSCAN(metric='cosine', eps=0.1, min_samples=1)\n",
    "df['cluster'] = dbscan.fit_predict(vectors)\n",
    "\n",
    "# Fonction pour obtenir le nom le plus fréquent dans chaque cluster\n",
    "def get_representative_name(cluster_df):\n",
    "    return cluster_df['client_names'].mode()[0]\n",
    "\n",
    "# Harmonisation des noms par cluster\n",
    "harmonized_names = df.groupby('cluster').apply(get_representative_name)\n",
    "harmonized_names = harmonized_names.to_dict()\n",
    "\n",
    "# Appliquer les noms harmonisés au DataFrame original\n",
    "df['harmonized_name'] = df['cluster'].map(harmonized_names)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(df[['client_names', 'corrected_names', 'harmonized_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e442413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Obtaining dependency information for fuzzywuzzy from https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d3dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/anaconda3/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/macbookpro/nltk_data'\n    - '/Users/macbookpro/anaconda3/nltk_data'\n    - '/Users/macbookpro/anaconda3/share/nltk_data'\n    - '/Users/macbookpro/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Franck Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Ismael Franck\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsmael Franck KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFranck Ismael KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASI Franck Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUSSI Franck Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Frack Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Franck Ismal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFranck KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsmael KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI I. Franck\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Prétraiter les noms\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m preprocessed_names \u001b[38;5;241m=\u001b[39m [preprocess_name(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Vectorisation TF-IDF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Franck Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Ismael Franck\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsmael Franck KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFranck Ismael KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASI Franck Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUSSI Franck Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Frack Ismael\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI Franck Ismal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFranck KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIsmael KOUASSI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKOUASSI I. Franck\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Prétraiter les noms\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m preprocessed_names \u001b[38;5;241m=\u001b[39m [preprocess_name(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Vectorisation TF-IDF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mpreprocess_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_name\u001b[39m(name):\n\u001b[1;32m      9\u001b[0m     name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 10\u001b[0m     name \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(name)\n\u001b[1;32m     11\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(name)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/macbookpro/nltk_data'\n    - '/Users/macbookpro/anaconda3/nltk_data'\n    - '/Users/macbookpro/anaconda3/share/nltk_data'\n    - '/Users/macbookpro/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "\n",
    "# Prétraitement\n",
    "def preprocess_name(name):\n",
    "    name = name.lower()\n",
    "    name = nltk.word_tokenize(name)\n",
    "    name = sorted(name)\n",
    "    return \" \".join(name)\n",
    "\n",
    "# Similarité basée sur FuzzyWuzzy\n",
    "def calculate_similarity(name1, name2):\n",
    "    return fuzz.token_sort_ratio(name1, name2)\n",
    "\n",
    "# Charger les noms\n",
    "names = [\n",
    "    \"KOUASSI Franck Ismael\", \"KOUASSI Ismael Franck\", \"Ismael Franck KOUASSI\",\n",
    "    \"Franck Ismael KOUASSI\", \"KOUASI Franck Ismael\", \"KOUSSI Franck Ismael\",\n",
    "    \"KOUASSI Frack Ismael\", \"KOUASSI Franck Ismal\", \"Franck KOUASSI\", \"Ismael KOUASSI\", \"KOUASSI I. Franck\"\n",
    "]\n",
    "\n",
    "# Prétraiter les noms\n",
    "preprocessed_names = [preprocess_name(name) for name in names]\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_names)\n",
    "\n",
    "# Clustering avec DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# Résultats\n",
    "name_clusters = pd.DataFrame({'name': names, 'cluster': clusters})\n",
    "print(name_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87adc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
